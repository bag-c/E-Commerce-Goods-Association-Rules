import pandas as pd
from apyori import apriori
import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split

# ========== ç¬¬ä¸€éƒ¨åˆ†ï¼šä½ çš„åŸAprioriä»£ç  ==========
# å¯¼å…¥æ•°æ®
data = pd.read_csv(r"E:\ç”µå•†RFMæ¨¡å‹\goods\GoodsOrder.csv")

# å¤„ç†ç¼ºå¤±å€¼
data.drop(index=data[data['Goods'].isnull()].index, inplace=True)

# å¤„ç†å¼‚å¸¸å€¼
data.drop(index=data[data['id']<0].index, inplace=True)

# è®¡ç®—å…³è”è§„åˆ™
length = data['id'].value_counts().count()
data_list = []

for i in range(1, length):
    item = data[data['id']==i]
    item_list = list(item['Goods'])
    data_list.append(item_list)

# è®¡ç®—å…³è”è§„åˆ™
rules = apriori(data_list, min_support=0.01, min_confidence=0.5)

relationship_list = []
for rule in rules:
    support = round(rule.support, 3)
    for i in rule.ordered_statistics:
        if i.lift > 2:
            head_set = list(i.items_base)
            head_tail = list(i.items_add)
            related_category = str(head_set) + '->' + str(head_tail)
            confidence = round(i.confidence, 3)
            lift = round(i.lift, 3)
    relationship_list.append([related_category, support, confidence, lift])

df_rules = pd.DataFrame(relationship_list, columns=['å…³è”è§„åˆ™', 'æ”¯æŒåº¦', 'ç½®ä¿¡åº¦', 'æå‡åº¦'])

print("=== Aprioriå…³è”è§„åˆ™ç»“æœ ===")
print(df_rules.head())

# ========== ç¬¬äºŒéƒ¨åˆ†ï¼šç»“åˆå†³ç­–æ ‘ ==========
print("\n" + "="*60)
print("å¼€å§‹ç»“åˆå†³ç­–æ ‘è¿›è¡Œç”¨æˆ·è´­ä¹°é¢„æµ‹")
print("="*60)

# 1. é€‰å–æœ€é‡è¦çš„5æ¡å…³è”è§„åˆ™ä½œä¸ºç‰¹å¾
top_rules = df_rules.nlargest(5, 'æå‡åº¦')
print(f"\né€‰å–çš„5æ¡é«˜æå‡åº¦è§„åˆ™ï¼š")
for i, (_, rule) in enumerate(top_rules.iterrows()):
    print(f"  è§„åˆ™{i+1}: {rule['å…³è”è§„åˆ™']} (æå‡åº¦: {rule['æå‡åº¦']:.2f})")

# 2. ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‰¹å¾å‘é‡
print("\nåˆ›å»ºæœºå™¨å­¦ä¹ ç‰¹å¾...")
user_features = []
all_user_ids = data['id'].unique()

for user_id in all_user_ids:
    # è·å–è¯¥ç”¨æˆ·è´­ä¹°çš„æ‰€æœ‰å•†å“
    user_items = set(data[data['id'] == user_id]['Goods'])
    
    features = []
    # å¯¹æ¯æ¡é‡è¦è§„åˆ™ï¼Œæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ»¡è¶³å‰ä»¶
    for _, rule_row in top_rules.iterrows():
        rule_str = rule_row['å…³è”è§„åˆ™']
        antecedent = eval(rule_str.split('->')[0])  # è·å–å‰ä»¶å•†å“åˆ—è¡¨
        
        # ç‰¹å¾ï¼šç”¨æˆ·æ˜¯å¦è´­ä¹°äº†å‰ä»¶å•†å“ï¼ˆ0/1ï¼‰
        has_antecedent = 1 if set(antecedent).issubset(user_items) else 0
        features.append(has_antecedent)
    
    user_features.append([user_id] + features)

# åˆ›å»ºç‰¹å¾DataFrame
feature_columns = ['ç”¨æˆ·ID'] + [f'è§„åˆ™{i+1}_å‰ä»¶' for i in range(len(top_rules))]
features_df = pd.DataFrame(user_features, columns=feature_columns)

# 3. åˆ›å»ºç›®æ ‡å˜é‡ï¼šç”¨æˆ·æ˜¯å¦è´­ä¹°äº†é«˜æå‡åº¦çš„åä»¶å•†å“
print("åˆ›å»ºç›®æ ‡å˜é‡...")
targets = []

for idx, user_id in enumerate(features_df['ç”¨æˆ·ID']):
    user_items = set(data[data['id'] == user_id]['Goods'])
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è´­ä¹°äº†ä»»æ„ä¸€æ¡é«˜æå‡åº¦è§„åˆ™çš„åä»¶
    bought_recommended = 0
    for _, rule_row in top_rules.iterrows():
        rule_str = rule_row['å…³è”è§„åˆ™']
        consequent = eval(rule_str.split('->')[1])  # è·å–åä»¶å•†å“
        
        if set(consequent).issubset(user_items):
            bought_recommended = 1
            break
    
    targets.append(bought_recommended)

# å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
X = features_df.drop('ç”¨æˆ·ID', axis=1)
y = np.array(targets)

print(f"\næ•°æ®é›†ä¿¡æ¯ï¼š")
print(f"  æ ·æœ¬æ•°: {X.shape[0]}ä¸ªç”¨æˆ·")
print(f"  ç‰¹å¾æ•°: {X.shape[1]}ä¸ªå…³è”è§„åˆ™ç‰¹å¾")
print(f"  æ­£æ ·æœ¬(è´­ä¹°æ¨èå•†å“): {sum(y)}äºº, è´Ÿæ ·æœ¬: {len(y)-sum(y)}äºº")

# 4. è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹
print("\nè®­ç»ƒå†³ç­–æ ‘æ¨¡å‹...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

tree_model = DecisionTreeClassifier(
    max_depth=3,  # é™åˆ¶æ·±åº¦ä¾¿äºè§£é‡Š
    min_samples_split=20,
    random_state=42
)

tree_model.fit(X_train, y_train)

# 5. è¯„ä¼°æ¨¡å‹
from sklearn.metrics import accuracy_score, classification_report

y_pred = tree_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\næ¨¡å‹è¯„ä¼°ç»“æœï¼š")
print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.3f}")
print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼š")
print(classification_report(y_test, y_pred, target_names=['æœªè´­ä¹°', 'è´­ä¹°']))

# 6. å¯è§†åŒ–å†³ç­–æ ‘
print("\nç”Ÿæˆå†³ç­–æ ‘å¯è§†åŒ–...")
plt.figure(figsize=(15, 8))
plot_tree(tree_model, 
          feature_names=[f'è§„åˆ™{i+1}' for i in range(X.shape[1])],
          class_names=['ä¸è´­ä¹°', 'è´­ä¹°'],
          filled=True,
          rounded=True,
          fontsize=10)
plt.title('å…³è”è§„åˆ™è´­ä¹°é¢„æµ‹å†³ç­–æ ‘', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# 7. ç‰¹å¾é‡è¦æ€§åˆ†æ
print("\n" + "="*60)
print("ç‰¹å¾é‡è¦æ€§åˆ†æ")
print("="*60)

feature_importance = pd.DataFrame({
    'ç‰¹å¾': [f'è§„åˆ™{i+1}' for i in range(X.shape[1])],
    'é‡è¦æ€§': tree_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

print(feature_importance)

# 8. ä¸šåŠ¡è§£è¯»
print("\n" + "="*60)
print("ä¸šåŠ¡æ´å¯Ÿä¸åº”ç”¨å»ºè®®")
print("="*60)

for idx, row in feature_importance.iterrows():
    if row['é‡è¦æ€§'] > 0:
        rule_idx = int(row['ç‰¹å¾'].replace('è§„åˆ™', '')) - 1
        rule = top_rules.iloc[rule_idx]
        
        antecedent = rule['å…³è”è§„åˆ™'].split('->')[0]
        consequent = rule['å…³è”è§„åˆ™'].split('->')[1]
        
        print(f"\nğŸ“Œ å…³é”®è§„åˆ™ {rule_idx+1}:")
        print(f"   å…³è”å…³ç³»: {antecedent} â†’ {consequent}")
        print(f"   ç‰¹å¾é‡è¦æ€§: {row['é‡è¦æ€§']:.3f}")
        print(f"   æ¨èç­–ç•¥: å¯¹è´­ä¹°äº†{antecedent}çš„ç”¨æˆ·ï¼Œé‡ç‚¹æ¨è{consequent}")
        print(f"   é¢„æœŸæ•ˆæœ: æå‡åº¦ {rule['æå‡åº¦']:.1f}å€")

# 9. é¢„æµ‹ç¤ºä¾‹
print("\n" + "="*60)
print("é¢„æµ‹ç¤ºä¾‹")
print("="*60)

print("ç¤ºä¾‹ç”¨æˆ·ç‰¹å¾å‘é‡ï¼ˆ0=æœªè´­ä¹°å‰ä»¶ï¼Œ1=è´­ä¹°äº†å‰ä»¶ï¼‰ï¼š")
sample_features = pd.DataFrame([X.iloc[0]], columns=X.columns)
print(sample_features)

sample_pred = tree_model.predict(sample_features)[0]
pred_proba = tree_model.predict_proba(sample_features)[0]

print(f"\né¢„æµ‹ç»“æœï¼š")
print(f"  æ˜¯å¦è´­ä¹°æ¨èå•†å“: {'æ˜¯' if sample_pred == 1 else 'å¦'}")
print(f"  è´­ä¹°æ¦‚ç‡: {pred_proba[1]:.1%}")

# ========== åŸå¯è§†åŒ–éƒ¨åˆ†ä¿æŒ ==========
print("\n" + "="*60)
print("å…³è”è§„åˆ™æå‡åº¦æ’åºå¯è§†åŒ–")
print("="*60)

plt.rcParams['font.sans-serif'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False

df_sort = df_rules.sort_values(by='æå‡åº¦', ascending=False).head(10)  # åªæ˜¾ç¤ºå‰10
y_values = df_sort['æå‡åº¦']
x_pos = range(len(df_sort))
x_labels = df_sort['å…³è”è§„åˆ™']

plt.figure(figsize=(12, 6))
bars = plt.bar(x=x_pos, height=y_values, width=0.6, color='skyblue')

plt.xticks(x_pos, x_labels, rotation=45, ha='right', fontsize=9)
plt.xlabel('å•†å“å…³è”è§„åˆ™', fontsize=14, fontweight='bold')
plt.ylabel('æå‡åº¦', fontsize=14, fontweight='bold')
plt.title('TOP 10 å•†å“å…³è”è§„åˆ™æå‡åº¦æ’åº', fontsize=14, fontweight='bold')
plt.grid(axis='y', alpha=0.3)

for i, bar in enumerate(bars):
    height = bar.get_height()
    bar_x = bar.get_x() + bar.get_width() / 2
    bar_y = height + 0.05
    plt.text(bar_x, bar_y, f'{height:.2f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

print("\nâœ… åˆ†æå®Œæˆï¼Apriori + å†³ç­–æ ‘ç»“åˆåˆ†æå·²å®Œæˆã€‚")
print(f"   å‘ç°æœ‰æ•ˆå…³è”è§„åˆ™: {len(df_rules)} æ¡")
print(f"   æ„å»ºé¢„æµ‹æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.3f}")
print(f"   è¯†åˆ«å…³é”®ä¸šåŠ¡è§„åˆ™: {len(feature_importance[feature_importance['é‡è¦æ€§']>0])} æ¡")
